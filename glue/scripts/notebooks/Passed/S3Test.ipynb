{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59403f3e-fee2-4c86-9b63-2261a87d10bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/28 22:00:20 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+--------------------+--------------------+-----------+\n",
      "|     customer_id|             name|               email|             address|signup_date|\n",
      "+----------------+-----------------+--------------------+--------------------+-----------+\n",
      "|            2975|   Jordan Jackson|robertgates@examp...|  Unit 4331 Box 0522|       null|\n",
      "|   DPO AA 94940\"|       2022-07-08|                null|                null|       null|\n",
      "|            7681|  Mr. John Cooper|chelseajones@exam...|488 Jonathan Stra...|       null|\n",
      "|    South Cheryl|        PA 15104\"|          2023-05-05|                null|       null|\n",
      "|            2453|       Vicki Best| stacy01@example.com| 101 Huffman Squares|       null|\n",
      "|    South Dustin|        TN 46652\"|          2022-09-13|                null|       null|\n",
      "|            1848|Catherine Roberts|  jose20@example.org|2572 Katherine Manor|       null|\n",
      "|      Paigeshire|        NV 54827\"|          2020-06-19|                null|       null|\n",
      "|            8064|       Mark Henry|cobbphillip@examp...|608 Stephanie Val...|       null|\n",
      "|Port Samuelmouth|        ND 66190\"|          2024-02-13|                null|       null|\n",
      "|            6580|     Beth Burnett|charlesalvarado@e...|46113 Jasmine Pas...|       null|\n",
      "| Port Rebekahton|        MP 33602\"|          2020-06-24|                null|       null|\n",
      "|            3698|   Emily Anderson| fkramer@example.com|         USNV Watson|       null|\n",
      "|   FPO AE 81925\"|       2023-08-28|                null|                null|       null|\n",
      "|            4276|   Daniel Sanders|   lchen@example.org|  87808 Juarez Trail|       null|\n",
      "|    Whitechester|        MP 63024\"|          2024-01-05|                null|       null|\n",
      "|            8587|      Joseph Long|katelyncastillo@e...|     405 Black Lodge|       null|\n",
      "| Lake Kaylashire|        CA 69190\"|          2024-05-22|                null|       null|\n",
      "|            6101|   Nathan Rodgers| brian30@example.org|747 Wallace Villages|       null|\n",
      "|  Alexandershire|        AL 28627\"|          2020-05-15|                null|       null|\n",
      "+----------------+-----------------+--------------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Define parameters in a dictionary\n",
    "parameters = {\n",
    "    'JOB_NAME': 'FullLoadAndCDCProcesserJob',\n",
    "    'RAW_S3_PATH': 's3://ecommerce-data-lake-us-east-1-dev/01_raw/',\n",
    "    'STAGING_S3_PATH': 's3://ecommerce-data-lake-us-east-1-dev/02_staging/',\n",
    "    'PREPROCESSED_S3_PATH': 's3://ecommerce-data-lake-us-east-1-dev/03_preprocessed/',\n",
    "    'MASTER_S3_PATH': 's3://ecommerce-data-lake-us-east-1-dev/04_master/',\n",
    "    'CURATED_S3_PATH': 's3://ecommerce-data-lake-us-east-1-dev/06_curated/',\n",
    "    'TABLE_NAME': 'orders',\n",
    "    'PROCESS_TYPE': 'full_load',\n",
    "    'SOURCE': 'netSuite'\n",
    "}\n",
    "\n",
    "# Mock function to simulate getResolvedOptions\n",
    "def get_resolved_options(args, keys):\n",
    "    return {key: parameters[key] for key in keys}\n",
    "\n",
    "# Mock sys.argv to simulate Glue job arguments\n",
    "sys.argv = [\n",
    "    '--JOB_NAME', parameters['JOB_NAME'],\n",
    "    '--RAW_S3_PATH', parameters['RAW_S3_PATH'],\n",
    "    '--STAGING_S3_PATH', parameters['STAGING_S3_PATH'],\n",
    "    '--PREPROCESSED_S3_PATH', parameters['PREPROCESSED_S3_PATH'],\n",
    "    '--MASTER_S3_PATH', parameters['MASTER_S3_PATH'],\n",
    "    '--CURATED_S3_PATH', parameters['CURATED_S3_PATH'],\n",
    "    '--TABLE_NAME', parameters['TABLE_NAME'],\n",
    "    '--PROCESS_TYPE', parameters['PROCESS_TYPE'],\n",
    "    '--SOURCE', parameters['SOURCE']\n",
    "]\n",
    "\n",
    "# Use the mock get_resolved_options function\n",
    "args = get_resolved_options(sys.argv, [\n",
    "    'JOB_NAME', \n",
    "    'RAW_S3_PATH', \n",
    "    'STAGING_S3_PATH', \n",
    "    'PREPROCESSED_S3_PATH', \n",
    "    'MASTER_S3_PATH',\n",
    "    'CURATED_S3_PATH', \n",
    "    'TABLE_NAME', \n",
    "    'PROCESS_TYPE',\n",
    "    'SOURCE'\n",
    "])\n",
    "\n",
    "# Paths to JAR files\n",
    "hadoop_aws_jar_path = \"/opt/glue/jars/hadoop-aws-3.2.0.jar\"\n",
    "aws_sdk_jar_path = \"/opt/glue/jars/aws-java-sdk-bundle-1.11.375.jar\"\n",
    "\n",
    "# Initialize Spark session with S3 configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.jars\", \",\".join([hadoop_aws_jar_path, aws_sdk_jar_path])) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"--add-opens java.base/java.nio=ALL-UNNAMED --add-opens java.base/sun.nio.ch=ALL-UNNAMED\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session initialized successfully\")\n",
    "\n",
    "# Read source data\n",
    "source_df = spark.read.format(\"csv\").option(\"header\", \"true\").load('s3a://ecommerce-data-lake-730335322582-us-east-1-dev/01_raw/netsuite/customers/full_load/ingestion_date=2024062815/customers_2024062815.csv')\n",
    "\n",
    "# Show the data\n",
    "source_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0643ef-d240-4e72-8932-d3f00e67601c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
