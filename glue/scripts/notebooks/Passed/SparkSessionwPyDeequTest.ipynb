{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4299afb1-c818-4584-be31-0da8375330cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/28 22:01:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/28 22:01:19 WARN DependencyUtils: Local jar /opt/glue/jars/deequ-glue-1.0-SNAPSHOT-jar-with-dependencies.jar does not exist, skipping.\n",
      "24/06/28 22:01:20 INFO SparkContext: Running Spark version 3.4.0\n",
      "24/06/28 22:01:20 INFO ResourceUtils: ==============================================================\n",
      "24/06/28 22:01:20 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/06/28 22:01:20 INFO ResourceUtils: ==============================================================\n",
      "24/06/28 22:01:20 INFO SparkContext: Submitted application: pyspark-shell\n",
      "24/06/28 22:01:20 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/06/28 22:01:20 INFO ResourceProfile: Limiting resource is cpu\n",
      "24/06/28 22:01:20 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/06/28 22:01:20 INFO SecurityManager: Changing view acls to: root\n",
      "24/06/28 22:01:20 INFO SecurityManager: Changing modify acls to: root\n",
      "24/06/28 22:01:20 INFO SecurityManager: Changing view acls groups to: \n",
      "24/06/28 22:01:20 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/06/28 22:01:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
      "24/06/28 22:01:20 INFO Utils: Successfully started service 'sparkDriver' on port 34821.\n",
      "24/06/28 22:01:20 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/06/28 22:01:20 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/06/28 22:01:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/06/28 22:01:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/06/28 22:01:20 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/06/28 22:01:20 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ac90a734-f0ef-4066-9847-79b32b1c5ea8\n",
      "24/06/28 22:01:20 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "24/06/28 22:01:20 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/06/28 22:01:21 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/06/28 22:01:21 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/06/28 22:01:21 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/06/28 22:01:21 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "24/06/28 22:01:21 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "24/06/28 22:01:21 INFO Utils: Successfully started service 'SparkUI' on port 4044.\n",
      "24/06/28 22:01:21 INFO SparkContext: Added JAR /opt/glue/jars/iceberg-spark3-runtime-0.12.0.jar at spark://e010978b507a:34821/jars/iceberg-spark3-runtime-0.12.0.jar with timestamp 1719612080171\n",
      "24/06/28 22:01:21 INFO SparkContext: Added JAR /opt/glue/jars/hadoop-aws-3.2.0.jar at spark://e010978b507a:34821/jars/hadoop-aws-3.2.0.jar with timestamp 1719612080171\n",
      "24/06/28 22:01:21 INFO SparkContext: Added JAR /opt/glue/jars/aws-java-sdk-bundle-1.11.375.jar at spark://e010978b507a:34821/jars/aws-java-sdk-bundle-1.11.375.jar with timestamp 1719612080171\n",
      "24/06/28 22:01:21 ERROR SparkContext: Failed to add /opt/glue/jars/deequ-glue-1.0-SNAPSHOT-jar-with-dependencies.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /opt/glue/jars/deequ-glue-1.0-SNAPSHOT-jar-with-dependencies.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1968)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2023)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:507)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "24/06/28 22:01:21 INFO Executor: Starting executor ID driver on host e010978b507a\n",
      "24/06/28 22:01:21 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "24/06/28 22:01:21 INFO Executor: Fetching spark://e010978b507a:34821/jars/hadoop-aws-3.2.0.jar with timestamp 1719612080171\n",
      "24/06/28 22:01:21 INFO TransportClientFactory: Successfully created connection to e010978b507a/172.18.0.2:34821 after 28 ms (0 ms spent in bootstraps)\n",
      "24/06/28 22:01:21 INFO Utils: Fetching spark://e010978b507a:34821/jars/hadoop-aws-3.2.0.jar to /tmp/spark-dcd24c3e-3fc4-4f66-9249-cea4917ce07e/userFiles-f5026940-e53a-4e1c-8441-80c1c62ed048/fetchFileTemp14779387413209265379.tmp\n",
      "24/06/28 22:01:21 INFO Executor: Adding file:/tmp/spark-dcd24c3e-3fc4-4f66-9249-cea4917ce07e/userFiles-f5026940-e53a-4e1c-8441-80c1c62ed048/hadoop-aws-3.2.0.jar to class loader\n",
      "24/06/28 22:01:21 INFO Executor: Fetching spark://e010978b507a:34821/jars/aws-java-sdk-bundle-1.11.375.jar with timestamp 1719612080171\n",
      "24/06/28 22:01:21 INFO Utils: Fetching spark://e010978b507a:34821/jars/aws-java-sdk-bundle-1.11.375.jar to /tmp/spark-dcd24c3e-3fc4-4f66-9249-cea4917ce07e/userFiles-f5026940-e53a-4e1c-8441-80c1c62ed048/fetchFileTemp2535165483685360843.tmp\n",
      "24/06/28 22:01:22 INFO Executor: Adding file:/tmp/spark-dcd24c3e-3fc4-4f66-9249-cea4917ce07e/userFiles-f5026940-e53a-4e1c-8441-80c1c62ed048/aws-java-sdk-bundle-1.11.375.jar to class loader\n",
      "24/06/28 22:01:22 INFO Executor: Fetching spark://e010978b507a:34821/jars/iceberg-spark3-runtime-0.12.0.jar with timestamp 1719612080171\n",
      "24/06/28 22:01:22 INFO Utils: Fetching spark://e010978b507a:34821/jars/iceberg-spark3-runtime-0.12.0.jar to /tmp/spark-dcd24c3e-3fc4-4f66-9249-cea4917ce07e/userFiles-f5026940-e53a-4e1c-8441-80c1c62ed048/fetchFileTemp1776580731189630777.tmp\n",
      "24/06/28 22:01:22 INFO Executor: Adding file:/tmp/spark-dcd24c3e-3fc4-4f66-9249-cea4917ce07e/userFiles-f5026940-e53a-4e1c-8441-80c1c62ed048/iceberg-spark3-runtime-0.12.0.jar to class loader\n",
      "24/06/28 22:01:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40099.\n",
      "24/06/28 22:01:22 INFO NettyBlockTransferService: Server created on e010978b507a:40099\n",
      "24/06/28 22:01:22 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/06/28 22:01:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, e010978b507a, 40099, None)\n",
      "24/06/28 22:01:22 INFO BlockManagerMasterEndpoint: Registering block manager e010978b507a:40099 with 434.4 MiB RAM, BlockManagerId(driver, e010978b507a, 40099, None)\n",
      "24/06/28 22:01:22 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, e010978b507a, 40099, None)\n",
      "24/06/28 22:01:22 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, e010978b507a, 40099, None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['SPARK_VERSION'] = '3.5.1'\n",
    "\n",
    "# Paths to JAR files\n",
    "iceberg_jar_path = \"/opt/glue/jars/iceberg-spark3-runtime-0.12.0.jar\"\n",
    "hadoop_aws_jar_path = \"/opt/glue/jars/hadoop-aws-3.2.0.jar\"\n",
    "aws_sdk_jar_path = \"/opt/glue/jars/aws-java-sdk-bundle-1.11.375.jar\"\n",
    "pydeequ_jar_path = \"/opt/glue/jars/deequ-glue-1.0-SNAPSHOT-jar-with-dependencies.jar\"\n",
    "\n",
    "# Initialize Spark session with Iceberg, S3, and PyDeequ configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.jars\", \",\".join([iceberg_jar_path, hadoop_aws_jar_path, aws_sdk_jar_path, pydeequ_jar_path])) \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
    "    .config(\"spark.sql.catalog.master_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.master_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.master_catalog.warehouse\", \"s3://ecommerce-data-lake-us-east-1-dev/04_master/\") \\\n",
    "    .config(\"spark.sql.catalog.curated_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.curated_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.curated_catalog.warehouse\", \"s3://ecommerce-data-lake-us-east-1-dev/06_curated/\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session initialized successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e0d870-9e24-4d13-b479-4ee92b845cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
